# -*- coding: utf-8 -*-
"""Retrieve Search Results.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ggIj7u8cwSKPAIpY6NV_P6Clz4cYB4Cu
"""
from bs4 import BeautifulSoup
import csv
import requests
import urllib.parse
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np


def is_quote_ok(s):
    stack = []
    for c in s:
        if c in ["'", '"', "`"]:
            if stack and stack[-1] == c:
                # this single-quote is close character
                stack.pop()
            else:
                # a new quote started
                stack.append(c)
        else:
            # ignore it
            pass
    if len(stack) > 0:
        print(f"s not good {s}")
        for c in stack:
            s += c
        print(f"new s {s}")
    return s


url = "https://repository.tudelft.nl/islandora/search/"

# Load the Excel file into a pandas dataframe
# df = pd.read_excel('data/Analytics repository.tudelft.nl Search Terms 20220101-20221231.xlsx', sheet_name='Dataset1')
df = pd.read_excel('data/Analytics Alle websitegegevens Search Terms 20220101-20221231.xlsx', sheet_name='Dataset1')
# Get the column you want to loop through
# restrict to 20 search terms in this test
column_data = df['Search Term']
# Create a dictionary to hold the search term and results
results_dict = {}

# Loop through the column
for search_term in column_data:
    print(f'searching for -> {search_term}')
    # search_term = "aula"  # for testing only
    search_term = is_quote_ok(search_term)
    search_term_encoded = urllib.parse.quote(search_term)
    search_collection = ["", "?collection=research", "?collection=education", "?collection=heritage"]

    save_csv = ["?display=tud_csv", "&display=tud_csv"]

    # search in all collections for now
    search_url = url + search_term_encoded + search_collection[0] + save_csv[0]

    response = requests.get(search_url)
    # Check if the response was successful
    if response.status_code == 200:
        # Open the response content as a string buffer
        csv_buffer = response.content.decode('utf-8')
        if "No search results, nothing to export!" in csv_buffer:
            # Handle the case where the CSV file does not exist or is empty
            print('no results found for this search term')
            # results_dict[search_term] = 'no results found'
        else:
            # Read url as a pandas dataframe
            print(search_url)
            try:
                csv_data = pd.read_csv(search_url)
                # results_dict[search_term] = csv_data
                csv_data.to_csv(f"output/{search_term.replace('/','')}.csv", sep="`")
            except pd.errors.ParserError:
                print("something went wrong")
    else:
        print('Failed to download CSV file')

# print(results_dict)

# # this code scrapes the top 20 results
# soup = BeautifulSoup(webpage.content, "html.parser")
# search_collection = soup.find('div', class_='islandora-solr-content')
#
#
# try:
#   # Extract the search results
#   search_results = search_collection.find_all("dl", class_="islandora-basic-collection-object")
#
#   # Loop through the search results and extract the position and uuid
#   for i, result in enumerate(search_results):
#       # Extract the uuid from the link
#       uuid = result.find("a")["href"].split("/")[-1]
#       uuid = urllib.parse.unquote(uuid)
#
#       # Extract the position from the class
#       # i.e. islandora-0, islandora-1, islandora-2

#       position = int(result["class"][1].split("-")[-1])
#
#       # print(f"Result {i+1}: UUID = {uuid}, Position = {position}")
#       # Add the uuid and position to the dictionary
#       if search_term not in results_dict:
#           results_dict[search_term] = []
#       results_dict[search_term].append({"position": position + 1, "uuid": uuid})
#
#   # Print the dictionary
#   print(results_dict)
#
#
# except AttributeError:
#     # Log the error
#     print(f"No search results found for {search_term}")
